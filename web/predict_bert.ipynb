{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell 3400\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_model(path + 'models/bert_model.h5', custom_objects=get_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(path + 'test1.csv', encoding='utf-8')\n",
    "\n",
    "review_data = data['review'].tolist()\n",
    "label_data = data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data['label'].isna().sum())\n",
    "data = data.dropna(subset=['label'])\n",
    "\n",
    "label_data = data['label'].astype(int).tolist()\n",
    "# print(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "tokenizer = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "replace_list = pickle.load(open(path+'vncorenlp/replace_list.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "from nltk import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_CHARS_LOWER = u'ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđð'\n",
    "VN_CHARS_UPPER = u'ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸÐĐ'\n",
    "VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER\n",
    "def no_marks(s):\n",
    "    __INTAB = [ch for ch in VN_CHARS]\n",
    "    __OUTTAB = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"*2\n",
    "    __OUTTAB += \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"*2\n",
    "    __r = re.compile(\"|\".join(__INTAB))\n",
    "    __replaces_dict = dict(zip(__INTAB, __OUTTAB))\n",
    "    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    \"anh\", \"chị\", \"bạn\", \"mình\", \"tôi\", \"ta\", \"ông\", \"họ\", \"em\", \"nó\",\n",
    "    \"chúng_ta\", \"chúng_tôi\", \"trên\", \"dưới\", \"trong\", \"ngoài\", \"giữa\",\n",
    "    \"của\", \"với\", \"tại\", \"đến\", \"qua\", \"vào\", \"bên\", \"và\", \"nhưng\",\n",
    "    \"hoặc\", \"mà\", \"vì\", \"bởi\", \"tuy_nhiên\", \"nếu\", \"rồi\", \"sau\",\n",
    "    \"sau_đó\", \"do\", \"tất_cả\", \"mỗi\", \"rất\", \"nhiều\", \"ít\", \"vài\",\n",
    "    \"hơn\", \"hết\", \"cả\", \"tất\", \"bây_giờ\", \"hôm_nay\", \"ngày\", \"đêm\",\n",
    "    \"trước\", \"hiện\", \"lúc\", \"khi\", \"đã\", \"vừa\", \"cái\", \"này\", \"kia\",\n",
    "    \"gì\", \"điều\", \"việc\", \"vậy\", \"thế\", \"là\", \"có\", \"được\", \"sẽ\",\n",
    "    \"làm\", \"như\", \"sao\", \"tại_sao\", \"thế_nào\", \"như_vậy\", \"vậy_nên\",\n",
    "    \"vậy_mà\", \"vậy_thì\", \"phải\", \"đấy\", \"đây\"\n",
    "]\n",
    "\n",
    "def preprocess(data):\n",
    "    token = []\n",
    "    for text in data:\n",
    "        check = re.search(r'([a-z])\\1+',text)\n",
    "        if check:\n",
    "          if len(check.group())>2:\n",
    "            text = re.sub(r'([a-z])\\1+', lambda m: m.group(1), text, flags=re.IGNORECASE) #remove các ký tự kéo dài như hayyy,ngonnnn...\n",
    "\n",
    "        text = text.strip() #loại dấu cách đầu câu\n",
    "        text = text.lower() #chuyển tất cả thành chữ thường\n",
    "\n",
    "        text = re.sub('< a class.+</a>',' ',text)\n",
    "\n",
    "        for k, v in replace_list.items():       #replace các từ có trong replace_list\n",
    "          text = text.replace(k, v)\n",
    "\n",
    "        text = re.sub(r'_',' ',text)\n",
    "\n",
    "        text = ' '.join(i for i in flatten(tokenizer.tokenize(text)))             #gán từ ghép\n",
    "\n",
    "        tokens = simple_preprocess(text)\n",
    "        filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "        text = ' '.join(filtered_tokens)\n",
    "\n",
    "        token.append(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = [str(review) for review in review_data if isinstance(review, str) or pd.notna(review)]\n",
    "data_test = preprocess(review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(path+'vncorenlp/vocab.txt', 'rb','utf-8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import Tokenizer\n",
    "tokenizer = Tokenizer(token_dict,cased=True)\n",
    "SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def load_data(data, sentiments):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    for text in data:\n",
    "        ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "\n",
    "    indices = np.array(indices)  # Chuyển đổi thành numpy array\n",
    "    return [indices, np.zeros_like(indices)], np.array(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test = load_data(data_test,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 296s 9s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
